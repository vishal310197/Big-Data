{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d181f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import findspark\n",
    "findspark.init('C:\\spark\\spark-3.1.2-bin-hadoop3.2')\n",
    "import pyspark\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60869958",
   "metadata": {},
   "source": [
    "# Three Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3528f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark.read.option(\"inferschema\",\"true\").option(\"header\",\"true\").csv(\"orders.csv\")\n",
    "customers = spark.read.option(\"inferschema\",\"true\").option(\"header\",\"true\").csv(\"customers.csv\")\n",
    "employees = spark.read.option(\"inferschema\",\"true\").option(\"header\",\"true\").csv(\"employees.csv\")\n",
    "orders.createTempView(\"orders\") \n",
    "customers.createTempView(\"customers\")\n",
    "employees.createTempView(\"employees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0a8b371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+-----------+----------+\n",
      "|order_id|customer_id|    status|salesman_id|order_date|\n",
      "+--------+-----------+----------+-----------+----------+\n",
      "|       1|          8|in process|          2|26-04-2010|\n",
      "|       2|          9|   pending|          3|26-04-2011|\n",
      "|       3|          9|   pending|          2|26-04-2012|\n",
      "+--------+-----------+----------+-----------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------+----+-------+--------+------------+\n",
      "|customer_id|name|address| website|credit_limit|\n",
      "+-----------+----+-------+--------+------------+\n",
      "|          1|Mike|    Usa|Mike.com|       77144|\n",
      "|          2|John|    Usa|John.com|       39943|\n",
      "|          3|Bray|    Usa|Bray.com|       64925|\n",
      "+-----------+----+-------+--------+------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------+----------+---------+-------------+-------+----------+----------+---------+\n",
      "|employee_id|first_name|last_name|        email|  phone| hire_date|manager_id|job_title|\n",
      "+-----------+----------+---------+-------------+-------+----------+----------+---------+\n",
      "|          1|     James|    Smith|  Smith@gmail|4628843|22-08-2005|         1|Developer|\n",
      "|          2|   Michael|     Rose|   Rose@gmail|  67853|22-08-2006|         2| Designer|\n",
      "|          3|    Robert|  William|William@gmail| 664829|22-08-2007|         3|   Editor|\n",
      "+-----------+----------+---------+-------------+-------+----------+----------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.show(3)\n",
    "customers.show(3)\n",
    "employees.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8185d512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+-----------+----------+-----------+------+-------+----------+------------+-----------+----------+---------+-------------+-------+----------+----------+---------+\n",
      "|order_id|customer_id|    status|salesman_id|order_date|customer_id|  name|address|   website|credit_limit|employee_id|first_name|last_name|        email|  phone| hire_date|manager_id|job_title|\n",
      "+--------+-----------+----------+-----------+----------+-----------+------+-------+----------+------------+-----------+----------+---------+-------------+-------+----------+----------+---------+\n",
      "|       1|          8|in process|          2|26-04-2010|          1|  Mike|    Usa|  Mike.com|       77144|          1|     James|    Smith|  Smith@gmail|4628843|22-08-2005|         1|Developer|\n",
      "|       2|          9|   pending|          3|26-04-2011|          2|  John|    Usa|  John.com|       39943|          2|   Michael|     Rose|   Rose@gmail|  67853|22-08-2006|         2| Designer|\n",
      "|       3|          9|   pending|          2|26-04-2012|          3|  Bray|    Usa|  Bray.com|       64925|          3|    Robert|  William|William@gmail| 664829|22-08-2007|         3|   Editor|\n",
      "|       4|         11|   pending|          5|26-04-2013|          4|Dwayne|    Usa|Dwayne.com|       65935|          4|     Maria|    Jones|  Jones@gmail|  67583|22-08-2008|         4|  Manager|\n",
      "|       5|         12|   arrived|          2|26-04-2014|          5| Scott|    Usa| Scott.com|       54935|          5|       Jen|   Browns| Browns@gmail|6278482|22-08-2009|         5|Team Lead|\n",
      "+--------+-----------+----------+-----------+----------+-----------+------+-------+----------+------------+-----------+----------+---------+-------------+-------+----------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#joining 3 tables\n",
    "orders.join(customers,orders.order_id==customers.customer_id).join(employees,customers.customer_id==employees.employee_id).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7195821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) BroadcastHashJoin [customer_id#42], [employee_id#68], LeftAnti, BuildRight, false\n",
      ":- *(3) BroadcastHashJoin [order_id#16], [customer_id#42], LeftOuter, BuildRight, false\n",
      ":  :- FileScan csv [order_id#16,customer_id#17,status#18,salesman_id#19,order_date#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/HI/big data/orders.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_id:int,customer_id:int,status:string,salesman_id:int,order_date:string>\n",
      ":  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#360]\n",
      ":     +- *(1) Filter isnotnull(customer_id#42)\n",
      ":        +- FileScan csv [customer_id#42,name#43,address#44,website#45,credit_limit#46] Batched: false, DataFilters: [isnotnull(customer_id#42)], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/HI/big data/customers.csv], PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<customer_id:int,name:string,address:string,website:string,credit_limit:int>\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#366]\n",
      "   +- *(2) Filter isnotnull(employee_id#68)\n",
      "      +- FileScan csv [employee_id#68] Batched: false, DataFilters: [isnotnull(employee_id#68)], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/HI/big data/employees.csv], PartitionFilters: [], PushedFilters: [IsNotNull(employee_id)], ReadSchema: struct<employee_id:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.join(customers,orders.order_id==customers.customer_id,'left_outer').join(employees,employees.employee_id==customers.customer_id,'left_anti').explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fdf0bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) BroadcastHashJoin [customer_id#42], [employee_id#68], LeftAnti, BuildRight, false\n",
      ":- *(3) BroadcastHashJoin [order_id#16], [customer_id#42], LeftOuter, BuildRight, false\n",
      ":  :- FileScan csv [order_id#16,customer_id#17,status#18,salesman_id#19,order_date#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/HI/big data/orders.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_id:int,customer_id:int,status:string,salesman_id:int,order_date:string>\n",
      ":  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#408]\n",
      ":     +- *(1) Filter isnotnull(customer_id#42)\n",
      ":        +- FileScan csv [customer_id#42,name#43,address#44,website#45,credit_limit#46] Batched: false, DataFilters: [isnotnull(customer_id#42)], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/HI/big data/customers.csv], PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<customer_id:int,name:string,address:string,website:string,credit_limit:int>\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#414]\n",
      "   +- *(2) Filter isnotnull(employee_id#68)\n",
      "      +- FileScan csv [employee_id#68] Batched: false, DataFilters: [isnotnull(employee_id#68)], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/HI/big data/employees.csv], PartitionFilters: [], PushedFilters: [IsNotNull(employee_id)], ReadSchema: struct<employee_id:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM orders LEFT OUTER JOIN customers ON orders.order_id==customers.customer_id LEFT ANTI JOIN employees ON customers.customer_id==employees.employee_id').explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "703ac41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[employee_id: int, first_name: string, last_name: string, email: string, phone: int, hire_date: string, manager_id: int, job_title: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#assignment 4\n",
    "#Which employees could convince customers to order products?\n",
    "#Not all employees are salseman\n",
    "#find where employee id = salesman id\n",
    "\n",
    "spark.sql('SELECT * FROM employees e LEFT SEMI JOIN orders o ON e.employee_id = o.salesman_id')\n",
    "employees.join(orders, f.expr(\"employee_id = salesman_id\"), \"left_semi\")\n",
    "\n",
    "#wrong\n",
    "#spark.sql('SELECT * FROM employees e LEFT OUTER JOIN orders o ON e.employee_id = o.salesman_id').show()\n",
    "#spark.sql('SELECT * FROM employees e LEFT ANTI JOIN orders o ON e.employee_id = o.salesman_id').show()\n",
    "#employees.join(orders, f.expr(\"employee_id = salesman_id\"), \"left_outer\").show()\n",
    "#employees.join(orders, f.expr(\"employee_id = salesman_id\"), \"left_anti\").show()\n",
    "#orders.join(employees, f.expr(\"employee_id = salesman_id\"), \"left_semi\").show()\n",
    "#orders.join(employees, f.expr(\"salesman_id = employee_id\"), \"left_anti\").show()\n",
    "#spark.sql('SELECT * FROM orders o LEFT SEMI JOIN employees e ON e.employee_id = o.salesman_id').show()\n",
    "#spark.sql('SELECT * FROM orders o LEFT ANTI JOIN employees e ON o.salesman_id = e.employee_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f06026c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[employee_id: int, sales: bigint]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#which employees could convince most customers?\n",
    "#only employess which are salesman should show\n",
    "\n",
    "employees.join(orders, f.expr(\"employee_id = salesman_id\"),\"inner\").groupBy(\"employee_id\").agg(f.expr(\"count(*) as sales\")).orderBy(\"sales\",ascending=False)\n",
    "spark.sql('SELECT e.employee_id, count(*) as sales FROM employees e INNER JOIN orders o ON e.employee_id = o.salesman_id GROUP BY e.employee_id ORDER BY sales DESC')\n",
    "\n",
    "#wrong\n",
    "#spark.sql('SELECT e.employee_id, count(*) as sales FROM employees e FULL JOIN orders o ON e.employee_id = o.salesman_id GROUP BY e.employee_id ORDER BY sales DESC')\n",
    "#spark.sql('SELECT e.employee_id, count(*) as sales FROM employees e INNER JOIN orders o ON e.employee_id = o.salesman_id GROUP BY o.salesman_id ORDER BY sales DESC')\n",
    "#spark.sql('SELECT e.employee_id, count(*) as sales FROM employees e INNER JOIN orders o ON e.employee_id = o.salesman_id GROUP BY e.employee_id ORDER BY sales DESC')\n",
    "#spark.sql('SELECT o.salesman_id, count(*) as sales FROM orders o LEFT ANTI JOIN employees e ON o.salesman_id = e.employee_id GROUP BY o.salesman_id ORDER BY sales')\n",
    "#spark.sql('SELECT e.employee_id, count(*) as sales FROM employees e SEMI JOIN orders o ON e.employee_id = o.salesman_id GROUP BY e.employee_id ORDER BY sales DESC')\n",
    "#employees.join(orders, f.expr(\"employee_id = salesman_id\"),\"full\").groupBy(\"employee_id\").agg(f.expr(\"count(*) as sales\")).orderBy(\"sales\",ascending=False)\n",
    "#employees.join(orders, f.expr(\"employee_id = salesman_id\"),\"left_semi\").groupBy(\"employee_id\").agg(f.expr(\"count(*) as sales\")).orderBy(\"sales\",ascending=False)\n",
    "#orders.join(employees, f.expr(\"salesman_id = employee_id\"),\"left_anti\").groupBy(\"salesman_id\").agg(f.expr(\"count(*) as sales\")).orderBy(\"sales\",ascending=True).show()\n",
    "#orders.join(employees, f.expr(\"salesman_id = employee_id\"),\"left_semi\").groupBy(\"employee_id\").agg(f.expr(\"count(*) as sales\")).orderBy(\"sales\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2c70ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d6409e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) Sort [year#208 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(year#208 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#174]\n",
      "   +- *(3) HashAggregate(keys=[year(cast(order_date#200 as date))#213], functions=[sum(cast((quantity#201 * unit_price#175) as bigint))])\n",
      "      +- Exchange hashpartitioning(year(cast(order_date#200 as date))#213, 200), ENSURE_REQUIREMENTS, [id=#170]\n",
      "         +- *(2) HashAggregate(keys=[year(cast(order_date#200 as date)) AS year(cast(order_date#200 as date))#213], functions=[partial_sum(cast((quantity#201 * unit_price#175) as bigint))])\n",
      "            +- *(2) Project [order_date#200, quantity#201, unit_price#175]\n",
      "               +- *(2) BroadcastHashJoin [order_id#196], [order_id#172], Inner, BuildRight, false\n",
      "                  :- *(2) Filter isnotnull(order_id#196)\n",
      "                  :  +- FileScan csv [order_id#196,order_date#200,quantity#201] Batched: false, DataFilters: [isnotnull(order_id#196)], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/HI/orders2.csv], PartitionFilters: [], PushedFilters: [IsNotNull(order_id)], ReadSchema: struct<order_id:int,order_date:string,quantity:int>\n",
      "                  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#164]\n",
      "                     +- *(1) Filter isnotnull(order_id#172)\n",
      "                        +- FileScan csv [order_id#172,unit_price#175] Batched: false, DataFilters: [isnotnull(order_id#172)], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/HI/order_items.csv], PartitionFilters: [], PushedFilters: [IsNotNull(order_id)], ReadSchema: struct<order_id:int,unit_price:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#assignment7\n",
    "order_items = spark.read.option(\"inferschema\",\"true\").option(\"header\",\"true\").csv(\"order_items.csv\")\n",
    "orders2 = spark.read.option(\"inferschema\",\"true\").option(\"header\",\"true\").csv(\"orders2.csv\")\n",
    "order_items.createTempView(\"order_items\") \n",
    "orders2.createTempView(\"orders2\") \n",
    "\n",
    "#physical plan\n",
    "spark.sql('SELECT year(order_date) as year, sum(quantity * unit_price) as revenue FROM orders2 NATURAL JOIN order_items GROUP BY year(order_date) ORDER BY year').explain()\n",
    "\n",
    "#wrong\n",
    "#spark.sql('SELECT order_date, sum(quantity * unit_price) as revenue FROM orders NATURAL JOIN order_items GROUP BY order_date ORDER BY order_date')\n",
    "#spark.sql('SELECT year(order_date) as year, sum(quantity * unit_price) as revenue FROM orders NATURAL JOIN order_items GROUP BY year(order_date) ORDER BY year Desc')\n",
    "#spark.sql('SELECT year(order_date) as year, sum(quantity) as revenue FROM orders NATURAL JOIN order_items GROUP BY year(order_date) ORDER BY year')\n",
    "#spark.sql('SELECT year(order_date) as year, sum(quantity * unit_price) as revenue FROM orders Full JOIN order_items GROUP BY year(order_date) ORDER BY year')\n",
    "#spark.sql('SELECT year(order_date) as year, sum(quantity + unit_price) as revenue FROM orders NATURAL JOIN order_items GROUP BY year(order_date) ORDER BY year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec247239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef51c9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 2), (11, 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rdd\n",
    "#Use the RDD API to identify how many orders each customer is waiting for (this means, status = \"Pending\"). \n",
    "#The customers are identified only by customer id. \n",
    "#Sort the resulting RDD by the number of pending orders in descending order.\n",
    "\n",
    "orders.rdd.filter(lambda row: row[\"status\"] == \"pending\").map(lambda row: (row[\"customer_id\"], 1)).reduceByKey(lambda x,y: x+y).sortBy(lambda x:x[1], ascending= False).collect()\n",
    "orders.rdd.filter(lambda y: y[\"status\"] == \"pending\").map(lambda y: (y[\"customer_id\"], 1)).reduceByKey(lambda x,y: x + y).sortBy(lambda x:x[1], ascending= False).collect()\n",
    "\n",
    "#orders.rdd.filter(lambda row: row[\"status\"] == \"pending\").map(lambda row: (row[\"customer_id\"], 1)).reduceByKey(lambda x,y: x * y).sortBy(lambda x:x[1], ascending= False)\n",
    "#orders.filter(lambda x: x[\"status\"] == \"pending\").map(lambda x: (x[\"customer_id\"], 2)).reduceByKey(lambda x,y: x+y).sortBy(lambda x:x[1], ascending= False)\n",
    "#orders.rdd.filter(lambda y: y[\"status == pending\"]).map(lambda y: (y[\"customer_id\"], 1)).reduceByKey(lambda x,y: x + y).sortBy(lambda x:x[1], ascending= False)\n",
    "#orders.filter(lambda row: row[\"status\"] == \"pending\").map(lambda row: (row[\"customer_id\"], 1)).reduceByKey(lambda x,y: x + y).sortBy(lambda x:x[1], ascending= False)\n",
    "#orders.rdd.filter(lambda y: y[\"status\"] == \"pending\").map(lambda y: (y[\"customer_id\"], 1)).reduceByKey(lambda x,y: x + y).sortBy(lambda x:x[2], ascending= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8b2cfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+-------------+-------+----------+----------+---------+\n",
      "|employee_id|first_name|last_name|        email|  phone| hire_date|manager_id|job_title|\n",
      "+-----------+----------+---------+-------------+-------+----------+----------+---------+\n",
      "|          1|     James|    Smith|  Smith@gmail|4628843|22-08-2005|         1|Developer|\n",
      "|          2|   Michael|     Rose|   Rose@gmail|  67853|22-08-2006|         2| Designer|\n",
      "|          3|    Robert|  William|William@gmail| 664829|22-08-2007|         3|   Editor|\n",
      "|          4|     Maria|    Jones|  Jones@gmail|  67583|22-08-2008|         4|  Manager|\n",
      "|          5|       Jen|   Browns| Browns@gmail|6278482|22-08-2009|         5|Team Lead|\n",
      "+-----------+----------+---------+-------------+-------+----------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f432d827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "BroadcastNestedLoopJoin BuildLeft, Inner\n",
      ":- BroadcastExchange IdentityBroadcastMode, [id=#209]\n",
      ":  +- FileScan csv [order_id#16,customer_id#17,status#18,salesman_id#19,order_date#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/HI/orders.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_id:int,customer_id:int,status:string,salesman_id:int,order_date:string>\n",
      "+- *(1) Filter ((isnotnull(employee_id#68) AND isnotnull(manager_id#74)) AND (employee_id#68 = manager_id#74))\n",
      "   +- FileScan csv [employee_id#68,first_name#69,last_name#70,email#71,phone#72,hire_date#73,manager_id#74,job_title#75] Batched: false, DataFilters: [isnotnull(employee_id#68), isnotnull(manager_id#74), (employee_id#68 = manager_id#74)], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/HI/employees.csv], PartitionFilters: [], PushedFilters: [IsNotNull(employee_id), IsNotNull(manager_id)], ReadSchema: struct<employee_id:int,first_name:string,last_name:string,email:string,phone:int,hire_date:string...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.join(employees, f.expr(\"employee_id = manager_id\"),\"inner\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e5eec08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [employee_id#68 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(employee_id#68 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#335]\n",
      "   +- BroadcastNestedLoopJoin BuildLeft, Inner\n",
      "      :- BroadcastExchange IdentityBroadcastMode, [id=#326]\n",
      "      :  +- FileScan csv [order_id#16,customer_id#17,status#18,salesman_id#19,order_date#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/HI/orders.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_id:int,customer_id:int,status:string,salesman_id:int,order_date:string>\n",
      "      +- *(1) Filter ((isnotnull(employee_id#68) AND isnotnull(manager_id#74)) AND (employee_id#68 = manager_id#74))\n",
      "         +- FileScan csv [employee_id#68,first_name#69,last_name#70,email#71,phone#72,hire_date#73,manager_id#74,job_title#75] Batched: false, DataFilters: [isnotnull(employee_id#68), isnotnull(manager_id#74), (employee_id#68 = manager_id#74)], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/HI/employees.csv], PartitionFilters: [], PushedFilters: [IsNotNull(employee_id), IsNotNull(manager_id)], ReadSchema: struct<employee_id:int,first_name:string,last_name:string,email:string,phone:int,hire_date:string...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM orders o INNER JOIN employees e ON e.employee_id = e.manager_id ORDER BY employee_id' ).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cb5f6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[employee_id: int, count: bigint]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.select(col('order_id'),col('salesman_id')).join(employees, f.expr(\"employee_id = salesman_id\"),\"left_outer\").groupBy(\"employee_id\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "253d233b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) Sort [year#337 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(year#337 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#301]\n",
      "   +- *(3) HashAggregate(keys=[year(cast(order_date#200 as date))#342], functions=[sum(cast((quantity#201 * unit_price#175) as bigint))])\n",
      "      +- Exchange hashpartitioning(year(cast(order_date#200 as date))#342, 200), ENSURE_REQUIREMENTS, [id=#297]\n",
      "         +- *(2) HashAggregate(keys=[year(cast(order_date#200 as date)) AS year(cast(order_date#200 as date))#342], functions=[partial_sum(cast((quantity#201 * unit_price#175) as bigint))])\n",
      "            +- *(2) Project [order_date#200, quantity#201, unit_price#175]\n",
      "               +- *(2) BroadcastHashJoin [order_id#196], [order_id#172], Inner, BuildRight, false\n",
      "                  :- *(2) Filter isnotnull(order_id#196)\n",
      "                  :  +- FileScan csv [order_id#196,order_date#200,quantity#201] Batched: false, DataFilters: [isnotnull(order_id#196)], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/HI/orders2.csv], PartitionFilters: [], PushedFilters: [IsNotNull(order_id)], ReadSchema: struct<order_id:int,order_date:string,quantity:int>\n",
      "                  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#291]\n",
      "                     +- *(1) Filter isnotnull(order_id#172)\n",
      "                        +- FileScan csv [order_id#172,unit_price#175] Batched: false, DataFilters: [isnotnull(order_id#172)], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/HI/order_items.csv], PartitionFilters: [], PushedFilters: [IsNotNull(order_id)], ReadSchema: struct<order_id:int,unit_price:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT year(order_date) as year, sum(quantity * unit_price) as revenue FROM orders2 NATURAL JOIN order_items GROUP BY year(order_date) ORDER BY year').explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0383107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('SELECT customer_id , title, first_name, last_name, email FROM Poll INNER JOIN Organizer ON organizer == organizer_id ORDER BY title ASC')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
